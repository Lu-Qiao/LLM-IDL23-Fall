{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "from typing import Tuple, Dict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import json\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "import torchsummary\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class UpgradeTokenizer:\n",
    "    def __init__(self, max_vocab_size, punctuations=['.', ',', '!', '?', ':', ';']):\n",
    "        self.vocab = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}\n",
    "        self.mask_token = '[MASK]'\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.punctuations = punctuations\n",
    "\n",
    "    def custom_tokenize(self, text):\n",
    "        # Escape punctuations for regular expression\n",
    "        escaped_punctuations = [re.escape(p) for p in self.punctuations]\n",
    "        # Pattern for words (including non-separated punctuations) or specified punctuations\n",
    "        pattern = r\"[^\\s\" + ''.join(escaped_punctuations) + r\"]+|\" + '|'.join(escaped_punctuations)\n",
    "        \n",
    "        tokens = re.findall(pattern, text)\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, corpus):\n",
    "        # Tokenize and count word frequencies\n",
    "        word_counts = Counter(word for sentence in corpus for word in self.custom_tokenize(sentence))\n",
    "        \n",
    "        # Select the most common words up to max_vocab_size\n",
    "        for word, _ in word_counts.most_common(self.max_vocab_size - len(self.vocab)):\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return [self.vocab.get(word, self.vocab['[UNK]']) for word in self.custom_tokenize(text)]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        words = [list(self.vocab.keys())[list(self.vocab.values()).index(token)] for token in tokens]\n",
    "        # sentence = ''\n",
    "        # for word in words:\n",
    "        #     if word in self.punctuations:\n",
    "        #         sentence += word  # Add punctuation without space\n",
    "        #     else:\n",
    "        #         if sentence and not sentence.endswith(' '):\n",
    "        #             sentence += ' '  # Add space before word if it's not the start of the sentence\n",
    "        #         sentence += word\n",
    "        sentence = \" \".join(words)\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = UpgradeTokenizer(max_vocab_size=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = '/home/luqiao/project/data/vocab40000-update.json'\n",
    "with open(vocab_file, 'r') as f:\n",
    "    VOCAB = json.load(f)\n",
    "\n",
    "tokenizer.vocab = VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in enumerate(tokenizer.vocab):\n",
    "    print(key, value)\n",
    "    if key ==100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenWebTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataset, partition, num_entries):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.data = self.dataset[partition][:num_entries][\"text\"]\n",
    "\n",
    "        self.max_seq_len = 128\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def mask_tokens(self, tokenized_lines, mask_probability=0.15):\n",
    "        inputs, labels = [], []\n",
    "        for token in tokenized_lines:\n",
    "            if random.random() < mask_probability and token != 1:\n",
    "                inputs.append(4)\n",
    "                labels.append(token) \n",
    "            else:\n",
    "                inputs.append(token)\n",
    "                labels.append(0) \n",
    "        return inputs, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        # print(idx, \"idx\")\n",
    "        # print(text, \"text check\")\n",
    "        encoded_text = self.tokenizer.tokenize(text)\n",
    "        # print(encoded_text, \"encode check\")\n",
    "        encoded_text.insert(0,2)\n",
    "        encoded_text.append(3)\n",
    "\n",
    "        inputs, labels = self.mask_tokens(encoded_text)\n",
    "\n",
    "        if len(inputs) > self.max_seq_len:\n",
    "            inputs = inputs[:self.max_seq_len]\n",
    "            labels = labels[:self.max_seq_len]\n",
    "\n",
    "        return torch.tensor(inputs, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def collate_fn(self,batch, max_seq_len = 128):\n",
    "\n",
    "        batch_x, batch_y, lengths_x, lengths_y = [], [], [], []\n",
    "\n",
    "        for x, y in batch:\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "            lengths_x.append(len(x))\n",
    "            lengths_y.append(len(y))\n",
    "\n",
    "        batch_x_pad = pad_sequence(batch_x, batch_first=True)# TODO\n",
    "        batch_y_pad = pad_sequence(batch_y, batch_first=True)# TODO\n",
    "\n",
    "\n",
    "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenwebDataset = datasets.load_from_disk(\"/home/luqiao/project/data/filtered_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenwebDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loding the training dataset.\n",
    "dataset     = np.load('/home/luqiao/project/data/tokenized_dataset_train.npy', allow_pickle=True)\n",
    "\n",
    "print(dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset     = np.load('/home/luqiao/project/data/tokenized_dataset_val.npy', allow_pickle=True)\n",
    "print(val_dataset[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict (\n",
    "    batch_size          = 96,\n",
    "    epochs              = 30,\n",
    "    learning_rate       = 3e-4,\n",
    "    weight_decay        = 5e-3,\n",
    "    tf_ratio                = 1.0,\n",
    "    patience                = 1,\n",
    ")\n",
    "\n",
    "with open('./config.json', 'w') as file:\n",
    "    json.dump(config, file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
    "\n",
    "    def __init__(self, dataset, batch_size, num_workers, seq_len = 128, shuffle= True, drop_last= False): \n",
    "        super(DataLoaderForLanguageModeling, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            # shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last\n",
    "\n",
    "        )\n",
    "        self.shuffle    = shuffle\n",
    "        # self.drop_last  = drop_last\n",
    "        self.seq_len = seq_len\n",
    "        self.l = len(np.concatenate(dataset))\n",
    "        self.num_batches = self.__len__()\n",
    "        # self.num_workers = num_workers\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return self.l//(self.batch_size*self.seq_len)\n",
    "        else:\n",
    "            return self.l//(self.batch_size*self.seq_len)+1\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            # TODO\n",
    "            np.random.shuffle(self.dataset)\n",
    "        all = np.concatenate(self.dataset)\n",
    "        # total_seq = (len(all)-1)// self.seq_len\n",
    "        padding_size = -len(all) % self.batch_size\n",
    "        padded_data = np.pad(all, (0, padding_size), mode='constant')\n",
    "\n",
    "        reshaped = padded_data.reshape(self.batch_size, -1)\n",
    "        targets = np.roll(reshaped, -1, axis=1)\n",
    "\n",
    "        leftover = len(all) % self.seq_len\n",
    "\n",
    "        batch_idx = 0\n",
    "        while batch_idx < self.num_batches:\n",
    "            start_idx = batch_idx * self.seq_len\n",
    "            end_idx = start_idx + self.seq_len\n",
    "            if batch_idx == self.num_batches - 1 and not self.drop_last:\n",
    "                end_idx = start_idx + leftover\n",
    "\n",
    "            batch_idx +=1\n",
    "\n",
    "            input = torch.tensor(reshaped[:, start_idx:end_idx], dtype=torch.long)\n",
    "            target = torch.tensor(targets[:, start_idx:end_idx], dtype= torch.long)\n",
    "\n",
    "            yield input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sanity checks\n",
    "\n",
    "dl = DataLoaderForLanguageModeling(\n",
    "    dataset     = dataset, \n",
    "    batch_size  = config[\"batch_size\"], \n",
    "    shuffle     = True, \n",
    "    drop_last   = True,\n",
    "    num_workers = 16,\n",
    "    # Input Extra parameters here if needed\n",
    ")\n",
    "\n",
    "# inputs, targets = next(iter(dl))\n",
    "\n",
    "# print(inputs.shape, targets.shape)\n",
    "\n",
    "\n",
    "# for x, y in dl:\n",
    "#     # print(x)\n",
    "#     print(\"x: \", tokenizer.convert_tokens_to_string(x[0, :]))\n",
    "#     print(\"y: \", tokenizer.convert_tokens_to_string(y[0, :]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_val = DataLoaderForLanguageModeling(\n",
    "    dataset     = val_dataset, \n",
    "    batch_size  = config[\"batch_size\"], \n",
    "    shuffle     = True, \n",
    "    drop_last   = True,\n",
    "    num_workers = 16,\n",
    "    # Input Extra parameters here if needed\n",
    ")\n",
    "\n",
    "inputs, targets = next(iter(dl))\n",
    "\n",
    "print(inputs.shape, targets.shape)\n",
    "\n",
    "# for i,j in iter(dl):\n",
    "#     print(i.shape, j.shape)\n",
    "\n",
    "for x, y in dl:\n",
    "    print(x)\n",
    "    print(\"x: \", tokenizer.convert_tokens_to_string(x[0, :]))\n",
    "    print(\"y: \", tokenizer.convert_tokens_to_string(y[0, :]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, value)\n",
    "    return output, attn\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.h = num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # Perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        # Transpose to get dimensions bs * h * sl * d_model\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        scores, attn = scaled_dot_product_attention(q, k, v, attn_mask)\n",
    "\n",
    "        # Concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, projection_size, max_seq_len= 128):\n",
    "        super().__init__()\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        denominator = torch.exp(torch.arange(0, projection_size, 2) * -(math.log(10000.0) / projection_size))\n",
    "        pe = torch.zeros(max_seq_len, projection_size, device=DEVICE)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * denominator)\n",
    "        pe[:, 1::2] = torch.cos(position * denominator)\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pos_encode',self.pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('input to pe', x[0])\n",
    "        # print('pe', self.pe.shape, self.pe[:, :x.size(1)].shape)\n",
    "        # print(self.pos_encode[0, :x.size(1)])\n",
    "        x = x + self.pos_encode[:, :x.size(1)]\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, projection_size, hidden_size, num_heads, dropout= 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # create the key, query and value weights\n",
    "        self.KW         = torch.nn.Linear(projection_size, projection_size)\n",
    "        self.VW         = torch.nn.Linear(projection_size, projection_size)\n",
    "        self.QW         = torch.nn.Linear(projection_size, projection_size)\n",
    "        # print(projection_size, num_heads)\n",
    "        self.attention = MultiHeadAttention(projection_size, num_heads)\n",
    "\n",
    "        self.bn1        = torch.nn.LayerNorm(projection_size)\n",
    "\n",
    "        self.bn2        = torch.nn.LayerNorm(projection_size)\n",
    "\n",
    "        # Feed forward neural network\n",
    "        self.MLP        = torch.nn.Sequential(\n",
    "            torch.nn.Linear(projection_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_size, projection_size)\n",
    "        )# TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        key     = self.KW(x)\n",
    "        value   = self.VW(x)\n",
    "        query   = self.QW(x)\n",
    "        \n",
    "        encoder_output, self_attention_weights   = self.attention(query,key,value)# TODO\n",
    "        out1    = encoder_output + x \n",
    "        # Apply batch norm to out1\n",
    "        out1    = self.bn1(out1)\n",
    "        \n",
    "        # Apply the output of the feed forward network\n",
    "        out2    = self.MLP(out1)\n",
    "        # Apply a residual connection between the input and output of the  FFN\n",
    "        out2    = out2 + out1\n",
    "        # Apply batch norm to the output\n",
    "        out2    = self.bn2(out2)\n",
    "\n",
    "        return out2, self_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                input_size,\n",
    "                encoder_embedding_size,\n",
    "                encoder_hidden_size,\n",
    "                n_heads,\n",
    "                tf_blocks,):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_size = encoder_embedding_size\n",
    "        self.embedding = nn.Embedding(input_size, encoder_embedding_size)\n",
    "\n",
    "        # compute the postion encoding\n",
    "        self.positional_encoding    = PositionalEncoding(encoder_embedding_size)# TODO\n",
    "\n",
    "        # create a sequence of transformer blocks\n",
    "        self.transformer_encoder    = torch.nn.ModuleList([TransformerEncoder(encoder_embedding_size, encoder_hidden_size, n_heads) for _ in range(tf_blocks)])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(encoder_embedding_size)\n",
    "\n",
    "        self.droupout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pass the output through the embedding\n",
    "        output                  = self.embedding(x)# TODO\n",
    "        # calculate the new output length\n",
    "        # output_lengths          = (output_lengths + 1 ) // 4 # TODO\n",
    "\n",
    "        # print(output.shape)\n",
    "        # calculate the position encoding\n",
    "        output  = self.positional_encoding(output)# TODO\n",
    "        output = self.droupout(output)\n",
    "\n",
    "\n",
    "        output = self.layer_norm(output)\n",
    "        # print(output.shape, output)\n",
    "        # Pass the output of the positional encoding through the transformer encoder\n",
    "        for encoder in self.transformer_encoder:\n",
    "            output, _ = encoder(output)# TODO\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, decoder_embedding_size, hidden_size, num_layers, num_heads, output_size, dropout):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, decoder_embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.positional_encoding    = PositionalEncoding(decoder_embedding_size)\n",
    "        self.layer_norm = nn.LayerNorm(decoder_embedding_size, eps=1e-6)\n",
    "        self.slf_attn = MultiHeadAttention(embedding_size=decoder_embedding_size, num_heads=num_heads)\n",
    "        self.enc_attn = MultiHeadAttention(embedding_size=decoder_embedding_size, num_heads=num_heads)\n",
    "\n",
    "        self.MLP        = torch.nn.Sequential(\n",
    "            torch.nn.Linear(decoder_embedding_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, decoder_embedding_size),\n",
    "        )# TODO\n",
    "        self.linear = nn.Linear(decoder_embedding_size, output_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(decoder_embedding_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, y=None, MAX_LENGTH=21):\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        if y == None:\n",
    "            y = torch.full((batch_size, 1), tokenizer.vocab['[CLS]'], device=DEVICE)\n",
    "            predictions = []\n",
    "            # attention_weights_all = []\n",
    "            # print(y.shape)\n",
    "            for t in range(MAX_LENGTH): \n",
    "                embedding  = self.embedding(y)# TODO\n",
    "                embedding = self.layer_norm(embedding)\n",
    "                # print(embedding.shape)\n",
    "\n",
    "                # embedding = torch.unsqueeze(self.dropout(self.embedding(y)), 1)\n",
    "                # context_vector, attention_weights = self.attention(embedding, encoder_outputs[:,t,:], encoder_outputs[:,t,:])\n",
    "                # output = torch.cat((context_vector, embedding), dim=2)\n",
    "                # prediction = torch.squeeze(self.fc_out(output), 1)\n",
    "\n",
    "                dec_output, dec_slf_attn = self.slf_attn(embedding, embedding, embedding)\n",
    "                # print(dec_output.shape, encoder_outputs.shape)\n",
    "                dec_output, dec_enc_attn = self.enc_attn(dec_output, encoder_outputs, encoder_outputs)\n",
    "                # dec_output = self.MLP(dec_output) + dec_output  # Apply residual connection\n",
    "                # dec_output = self.layer_norm2(dec_output)\n",
    "                # print(dec_output.shape)\n",
    "                prediction = self.linear(dec_output)\n",
    "\n",
    "                prediction = prediction[:, -1, :]\n",
    "                predictions.append(prediction)\n",
    "                # attention_weights_all.append((dec_slf_attn, dec_enc_attn))\n",
    "                # print(prediction.shape)\n",
    "                # if t == 0:\n",
    "                #     next_token = torch.argmax(prediction, dim=-1, keepdim=True)\n",
    "                # else:\n",
    "                next_token = torch.argmax(prediction, dim=-1, keepdim=True)\n",
    "\n",
    "                # print(next_token.shape)\n",
    "                # print(y.shape)\n",
    "                y = torch.cat([y, next_token], dim=1)\n",
    "\n",
    "            predictions = torch.stack(predictions, dim=1)  # Shape: (batch_size, MAX_LENGTH, output_size)\n",
    "            return predictions \n",
    "\n",
    "        # y shape: (batch_size, seq length)\n",
    "        y  = self.embedding(y)\n",
    "        y  = self.positional_encoding(y)\n",
    "\n",
    "        embedding = self.dropout(y)  # shape: (batch_size, 1, embedding_size)\n",
    "\n",
    "        embedding = self.layer_norm(embedding)\n",
    "\n",
    "        # dec_slf_attn_list, dec_enc_attn_list = [], []\n",
    "        dec_output, dec_slf_attn = self.slf_attn(embedding, embedding, embedding)\n",
    "        dec_output, dec_enc_attn = self.enc_attn(dec_output, encoder_outputs, encoder_outputs)\n",
    "        \n",
    "        # Apply the output of the feed forward network\n",
    "        out1    = self.MLP(dec_output) \n",
    "        # Apply a residual connection between the input and output of the  FFN\n",
    "        out2    = dec_output + out1 \n",
    "        # Apply batch norm to the output\n",
    "        out2    = self.layer_norm2(out2) \n",
    "\n",
    "        logits = self.linear(out2)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, encoder_embedding_size, encoder_hidden_size, encoder_n_heads, tf_blocks,\n",
    "                decoder_embedding_size, decoder_hidden_size, num_layers, num_heads, output_size, dropout = 0.1):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = Encoder(input_size,encoder_embedding_size, encoder_hidden_size, n_heads = encoder_n_heads, tf_blocks = tf_blocks)\n",
    "        self.decoder = DecoderWithAttention(input_size, decoder_embedding_size, decoder_hidden_size, num_layers, num_heads, output_size, dropout)\n",
    "\n",
    "    def forward(self, source, target=None):\n",
    "\n",
    "        encoder_outputs= self.encoder(source)\n",
    "\n",
    "        dec_output = self.decoder(encoder_outputs, target)\n",
    "\n",
    "        return dec_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict (\n",
    "    batch_size          = 128,\n",
    "    epochs              = 30,\n",
    "    learning_rate       = 3e-4,\n",
    "    weight_decay        = 5e-3,\n",
    "    encoder_embedding_size  = 256,\n",
    "    encoder_hidden_size     = 256,\n",
    "    encoder_n_heads         = 8,\n",
    "    tf_blocks               = 6,\n",
    "    vocab_size              = 40000,\n",
    "    decoder_embedding_size   = 256, \n",
    "    decoder_hidden_size     = 256,\n",
    "    num_layers              = 3,\n",
    "    num_heads               = 4,\n",
    "    tf_ratio                = 1.0,\n",
    "    patience                = 1,\n",
    ")\n",
    "\n",
    "with open('./model_config.json', 'w') as file:\n",
    "    json.dump(model_config, file, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqWithAttention(model_config[\"vocab_size\"], model_config['encoder_embedding_size'], model_config['encoder_hidden_size'], model_config['encoder_n_heads'], model_config['tf_blocks'],\n",
    "                model_config['decoder_embedding_size'], model_config['decoder_hidden_size'], model_config['num_layers'], model_config['num_heads'], model_config[\"vocab_size\"])\n",
    "model = model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchsummary\n",
    "# x_sample    = torch.rand(64, 128).long()\n",
    "# x_sample = x_sample.to(DEVICE)\n",
    "# y_sample = torch.rand(64, 128).long()\n",
    "# y_sample = y_sample.to(DEVICE)\n",
    "# # print(x_sample, y_sample)\n",
    "# torchsummary.summary(model, x_sample, y_sample)\n",
    "# del x_sample, y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_sample    = torch.rand(64, 128).long()\n",
    "# torchsummary.summary(model, x_sample.to(DEVICE))\n",
    "# del x_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_edit_distance(predictions, y,tokenizer, vocab= VOCAB, print_example= True):\n",
    "\n",
    "    dist                = 0\n",
    "    batch_size, seq_len = predictions.shape\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "\n",
    "        y_sliced    = tokenizer.convert_tokens_to_string(y[batch_idx])\n",
    "        pred_sliced = tokenizer.convert_tokens_to_string(predictions[batch_idx])\n",
    "        print(y_sliced)\n",
    "        print(pred_sliced)\n",
    "\n",
    "        # # Strings - When you are using characters from the AudioDataset\n",
    "        # y_string    = ''.join(y_sliced)\n",
    "        # pred_string = ''.join(pred_sliced)\n",
    "\n",
    "        dist        += Levenshtein.distance(pred_sliced, y_sliced)\n",
    "        # Comment the above abd uncomment below for toy dataset\n",
    "        # dist      += Levenshtein.distance(y_sliced, pred_sliced)\n",
    "\n",
    "    # if print_example:\n",
    "    #     # Print y_sliced and pred_sliced if you are using the toy dataset\n",
    "    #     print(\"\\nGround Truth : \", y_string)\n",
    "    #     print(\"Prediction   : \", pred_string)\n",
    "\n",
    "    dist    /= batch_size\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before optimizer:\", next(model.parameters()).device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding for loss calculation\n",
    "print(\"Before optimizer:\", next(model.parameters()).device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"After optimizer:\", next(model.parameters()).device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(criterion, out, target):\n",
    "\n",
    "    out     = out.view(-1, out.size(2))\n",
    "    targets = torch.flatten(target)\n",
    "    loss    = criterion(out, targets)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "\n",
    "        src = src.to(DEVICE)\n",
    "        trg = trg.to(DEVICE)\n",
    "\n",
    "\n",
    "        # with torch.cuda.amp.autocast():\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # trg shape: (batch_size, trg_len)\n",
    "        # output shape: (batch_size, trg_len, output_dim)\n",
    "\n",
    "        # output_dim = output.shape[-1]\n",
    "\n",
    "        # output = output[:, 1:].reshape(-1, output_dim)\n",
    "        # trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = calculate_loss(criterion, output, trg)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(epoch_loss/(i+1)),\n",
    "            # perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n",
    "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        batch_bar.update()\n",
    "\n",
    "        del src, trg\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    batch_bar.close()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
    "    running_dist = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, (src, trg) in enumerate(dataloader):\n",
    "\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "            \n",
    "            # print(tokenizer.convert_tokens_to_string(src[0]))\n",
    "            # print(tokenizer.convert_tokens_to_string(trg[0]))\n",
    "            # print(trg[0])\n",
    "            print(trg.shape, \"trg shape\")\n",
    "            \n",
    "            \n",
    "            output = model(src) # turn off teacher forcing\n",
    "            # trg = [trg len, batch size]\n",
    "            # output = [trg len, batch size, output dim]\n",
    "\n",
    "            # output_dim = output.shape[-1]\n",
    "            # output = output[1:].view(-1, output_dim)\n",
    "            # trg = trg[1:].view(-1)\n",
    "            print(output.shape)\n",
    "\n",
    "            # output = output.transpose(1,2)\n",
    "            \n",
    "            # loss    = criterion(output, targets)\n",
    "            # print(loss)\n",
    "            \n",
    "            prob_dist = torch.nn.functional.log_softmax(output, dim=-1)\n",
    "            pred_string = torch.argmax(prob_dist, dim=-1)\n",
    "            print(\"here\")\n",
    "            dist = calc_edit_distance(predictions=pred_string, y=trg, tokenizer=tokenizer)\n",
    "            running_dist += dist\n",
    "            # epoch_loss += loss.item()\n",
    "            batch_bar.set_postfix(\n",
    "                loss=\"{:.04f}\".format(epoch_loss/(i+1)))\n",
    "            batch_bar.update()\n",
    "            del src, trg\n",
    "            torch.cuda.empty_cache()\n",
    "            print(running_dist)\n",
    "    \n",
    "    batch_bar.close()\n",
    "    # print(running_dist)\n",
    "\n",
    "    return epoch_loss / len(dataloader), running_dist/ len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss = train(model, dl, optimizer, criterion, CLIP)\n",
    "    valid_loss, dist = validate(model, dl_val, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './best-model.pth')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f}')\n",
    "    print(f'\\t Distance: {dist:.5f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
