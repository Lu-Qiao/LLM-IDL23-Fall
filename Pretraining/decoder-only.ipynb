{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5: Large Language Model: NanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "from typing import Tuple, Dict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from datasets import load_dataset\n",
    "# import datasets\n",
    "import json\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "import torchsummary\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "class UpgradeTokenizer2:\n",
    "    def __init__(self, max_vocab_size, punctuations=['.', ',', '!', '?', ':', ';', '-', '(', ')']):\n",
    "        self.vocab = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}\n",
    "        self.mask_token = '[MASK]'\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.punctuations = punctuations\n",
    "\n",
    "    def custom_tokenize(self, text):\n",
    "        # Generate a regex pattern that excludes specified punctuations\n",
    "        # excluded_punctuations = ''.join(re.escape(p) for p in self.punctuations)\n",
    "        pattern = r\"\\b\\w+'?\\w*|[^\\w\\s]\"\n",
    "\n",
    "        tokens = re.findall(pattern, text.lower())\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        word_counts = Counter(word for sentence in corpus for word in self.custom_tokenize(sentence))\n",
    "        for word, _ in word_counts.most_common(self.max_vocab_size - len(self.vocab)):\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return [self.vocab.get(word, self.vocab['[UNK]']) for word in self.custom_tokenize(text)]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        words = [list(self.vocab.keys())[list(self.vocab.values()).index(token)] for token in tokens]\n",
    "        sentence = ''\n",
    "        for word in words:\n",
    "            if word in self.punctuations:\n",
    "                sentence += word\n",
    "            else:\n",
    "                if sentence and not sentence.endswith(' '):\n",
    "                    sentence += ' '\n",
    "                sentence += word\n",
    "        return sentence\n",
    "\n",
    "# Initialize your tokenizer\n",
    "tokenizer = UpgradeTokenizer2(max_vocab_size=60000)  # Adjust max_vocab_size as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_file = 'vocab60000-latest.json'\n",
    "with open(vocab_file, 'r') as f:\n",
    "    VOCAB = json.load(f)\n",
    "\n",
    "tokenizer.vocab = VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [PAD]\n",
      "1 [UNK]\n",
      "2 [CLS]\n",
      "3 [SEP]\n",
      "4 [MASK]\n",
      "5 the\n",
      "6 .\n",
      "7 ,\n",
      "8 to\n",
      "9 of\n",
      "10 and\n",
      "11 a\n",
      "12 in\n",
      "13 -\n",
      "14 that\n",
      "15 is\n",
      "16 ’\n",
      "17 for\n",
      "18 it\n",
      "19 on\n",
      "20 \"\n",
      "21 with\n",
      "22 s\n",
      "23 as\n",
      "24 was\n",
      "25 i\n",
      "26 :\n",
      "27 )\n",
      "28 (\n",
      "29 this\n",
      "30 be\n",
      "31 you\n",
      "32 are\n",
      "33 he\n",
      "34 at\n",
      "35 “\n",
      "36 ”\n",
      "37 by\n",
      "38 have\n",
      "39 from\n",
      "40 but\n",
      "41 not\n",
      "42 we\n",
      "43 an\n",
      "44 they\n",
      "45 his\n",
      "46 or\n",
      "47 has\n",
      "48 said\n",
      "49 will\n",
      "50 /\n",
      "51 their\n",
      "52 one\n",
      "53 more\n",
      "54 all\n",
      "55 can\n",
      "56 who\n",
      "57 about\n",
      "58 if\n",
      "59 which\n",
      "60 were\n",
      "61 had\n",
      "62 there\n",
      "63 so\n",
      "64 t\n",
      "65 when\n",
      "66 ?\n",
      "67 up\n",
      "68 would\n",
      "69 been\n",
      "70 out\n",
      "71 what\n",
      "72 ]\n",
      "73 [\n",
      "74 new\n",
      "75 also\n",
      "76 like\n",
      "77 —\n",
      "78 people\n",
      "79 time\n",
      "80 no\n",
      "81 your\n",
      "82 its\n",
      "83 some\n",
      "84 just\n",
      "85 than\n",
      "86 other\n",
      "87 my\n",
      "88 do\n",
      "89 1\n",
      "90 after\n",
      "91 our\n",
      "92 her\n",
      "93 into\n",
      "94 she\n",
      "95 first\n",
      "96 them\n",
      "97 ;\n",
      "98 –\n",
      "99 two\n",
      "100 only\n"
     ]
    }
   ],
   "source": [
    "for key, value in enumerate(tokenizer.vocab):\n",
    "    print(key, value)\n",
    "    if key ==100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loding the training dataset. Refer to write up section 2 to understand the structure\n",
    "all_train_dataset     = np.load('/root/all_train_tokenized_60000.npy', allow_pickle=True)\n",
    "\n",
    "print(all_train_dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataset     = np.load('/root/all_val_tokenized_60000.npy', allow_pickle=True)\n",
    "print(val_dataset[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = dict (\n",
    "    batch_size          = 64,\n",
    "    epochs              = 30,\n",
    "    lr       = 3e-5,\n",
    "    weight_decay        = 5e-3,\n",
    "    tf_ratio            = 1.0,\n",
    "    patience            = 1,\n",
    ")\n",
    "\n",
    "with open('/root/config.json', 'w') as file:\n",
    "    json.dump(config, file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    # TODO: You can probably add more parameters as well. Eg. sequence length\n",
    "    def __init__(self, dataset, batch_size, num_workers, seq_len = 512, shuffle= True, drop_last= False): \n",
    "        super(DataLoaderForLanguageModeling, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last\n",
    "\n",
    "        )\n",
    "        self.shuffle    = shuffle\n",
    "        self.seq_len = seq_len\n",
    "        self.l = len(np.concatenate(dataset))\n",
    "        self.num_batches = self.__len__()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return self.l//(self.batch_size*self.seq_len)\n",
    "        else:\n",
    "            return self.l//(self.batch_size*self.seq_len)+1\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            # TODO\n",
    "            np.random.shuffle(self.dataset)\n",
    "        all = np.concatenate(self.dataset)\n",
    "        padding_size = -len(all) % self.batch_size\n",
    "        padded_data = np.pad(all, (0, padding_size), mode='constant')\n",
    "\n",
    "        reshaped = padded_data.reshape(self.batch_size, -1)\n",
    "        targets = np.roll(reshaped, -1, axis=1)\n",
    "\n",
    "        leftover = len(all) % self.seq_len\n",
    "\n",
    "        batch_idx = 0\n",
    "        while batch_idx < self.num_batches:\n",
    "            start_idx = batch_idx * self.seq_len\n",
    "            end_idx = start_idx + self.seq_len\n",
    "            if batch_idx == self.num_batches - 1 and not self.drop_last:\n",
    "                end_idx = start_idx + leftover\n",
    "\n",
    "            batch_idx +=1\n",
    "\n",
    "            input = torch.tensor(reshaped[:, start_idx:end_idx], dtype=torch.long)\n",
    "            target = torch.tensor(targets[:, start_idx:end_idx], dtype= torch.long)\n",
    "\n",
    "            yield input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl = DataLoaderForLanguageModeling(\n",
    "    dataset     = all_train_dataset, \n",
    "    batch_size  = config[\"batch_size\"], \n",
    "    shuffle     = True, \n",
    "    drop_last   = True,\n",
    "    num_workers = 64,\n",
    "    # Input Extra parameters here if needed\n",
    ")\n",
    "\n",
    "inputs, targets = next(iter(dl))\n",
    "\n",
    "print(inputs.shape, targets.shape)\n",
    "\n",
    "\n",
    "for x, y in dl:\n",
    "    print(\"x: \", tokenizer.convert_tokens_to_string(x[0, :]))\n",
    "    print(\"y: \", tokenizer.convert_tokens_to_string(y[0, :]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl_val = DataLoaderForLanguageModeling(\n",
    "    dataset     = val_dataset, \n",
    "    batch_size  = config[\"batch_size\"], \n",
    "    shuffle     = False, \n",
    "    drop_last   = True,\n",
    "    num_workers = 64,\n",
    "    seq_len = 128\n",
    "    # Input Extra parameters here if needed\n",
    ")\n",
    "\n",
    "inputs, targets = next(iter(dl))\n",
    "\n",
    "print(inputs.shape, targets.shape)\n",
    "\n",
    "\n",
    "for x, y in dl:\n",
    "    print(x)\n",
    "    print(\"x: \", tokenizer.convert_tokens_to_string(x[0, :]))\n",
    "    print(\"y: \", tokenizer.convert_tokens_to_string(y[0, :]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, value)\n",
    "    return output, attn\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.h = num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # Perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        # Transpose to get dimensions bs * h * sl * d_model\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Calculate attention using function we will define next\n",
    "        scores, attn = scaled_dot_product_attention(q, k, v, attn_mask)\n",
    "\n",
    "        # Concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, projection_size, max_seq_len= 800):\n",
    "        super().__init__()\n",
    "        # Read the Attention Is All You Need paper to learn how to code code the positional encoding\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        denominator = torch.exp(torch.arange(0, projection_size, 2) * -(math.log(10000.0) / projection_size))\n",
    "        pe = torch.zeros(max_seq_len, projection_size, device=DEVICE)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * denominator)\n",
    "        pe[:, 1::2] = torch.cos(position * denominator)\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pos_encode',self.pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_encode[:, :x.size(1)]\n",
    "        return x\n",
    "    \n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, projection_size, hidden_size, num_heads, dropout= 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.attention = MultiHeadAttention(projection_size, num_heads)\n",
    "\n",
    "\n",
    "        self.bn1        = torch.nn.LayerNorm(projection_size)# TODO\n",
    "\n",
    "        self.bn2        = torch.nn.LayerNorm(projection_size)# TODO\n",
    "\n",
    "\n",
    "        # Feed forward neural network\n",
    "        self.MLP        = torch.nn.Sequential(\n",
    "            torch.nn.Linear(projection_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_size, projection_size)\n",
    "        )# TODO\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "\n",
    "\n",
    "        attention = self.attention(query, key, value, mask)\n",
    "\n",
    "        out1    = attention + query # TODO\n",
    "        \n",
    "        # Apply batch norm to out1\n",
    "        out1    = self.bn1(out1)# TODO\n",
    "        \n",
    "        # Apply the output of the feed forward network\n",
    "        out2    = self.MLP(out1) # TODO\n",
    "        # Apply a residual connection between the input and output of the  FFN\n",
    "        out2 = self.dropout(out2)\n",
    "        out2    = out2 + out1 # TODO\n",
    "        # Apply batch norm to the output\n",
    "        out2    = self.bn2(out2) # TODO\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                input_size,\n",
    "                embedding_size,\n",
    "                hidden_size,\n",
    "                output_size,\n",
    "                n_heads,\n",
    "                tf_blocks,\n",
    "                dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        # compute the postion encoding\n",
    "        self.positional_encoding    = PositionalEncoding(embedding_size)# TODO\n",
    "\n",
    "        # create a sequence of transformer blocks\n",
    "        self.transformer_blocks    = torch.nn.ModuleList([TransformerBlock(embedding_size, hidden_size, n_heads) for _ in range(tf_blocks)])\n",
    "\n",
    "        self.droupout1 = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, output_size)\n",
    "        self.droupout2 = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        # Pass the output through the embedding\n",
    "        output                  = self.embedding(x)# TODO\n",
    "        output = self.droupout1(output)\n",
    "        # calculate the position encoding\n",
    "        output  = self.positional_encoding(output)# TODO\n",
    "        output = self.droupout2(output)\n",
    "\n",
    "        output = self.layer_norm(output)\n",
    "\n",
    "        # Pass the output of the positional encoding through the transformer encoder\n",
    "        for block in self.transformer_blocks:\n",
    "            output = block(output, output, output, mask)# TODO\n",
    "\n",
    "        output = self.linear(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_heads, tf_blocks,dropout = 0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.decoder = Decoder(input_size, embedding_size, hidden_size, output_size, num_heads, tf_blocks, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "       mask = self.create_mask(x.size(1))\n",
    "\n",
    "       return self.decoder(x, mask)\n",
    "    \n",
    "    def generate(self, input_seq, max_length=150):\n",
    "        self.eval()\n",
    "        generated_seq = input_seq.to(DEVICE)\n",
    "        \n",
    "\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                logits  = self.forward(generated_seq)\n",
    "\n",
    "                # Get the last predicted token\n",
    "                predictions = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "                next_token = predictions[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "                generated_seq = torch.cat((generated_seq, next_token), dim=1)\n",
    "                \n",
    "\n",
    "        return generated_seq\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.tensor(x).long().to(DEVICE)\n",
    "        else: x = x.to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            predictions = self.forward(x)\n",
    "            \n",
    "            predictions = torch.nn.functional.log_softmax(predictions, dim=-1)\n",
    "\n",
    "\n",
    "            next_token = predictions[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "        \n",
    "        return next_token\n",
    "    \n",
    "\n",
    "    def create_mask(self, input_seq_length):\n",
    "        mask = torch.triu(torch.ones(input_seq_length, input_seq_length, device=DEVICE), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_edit_distance(predictions, y,tokenizer, vocab= VOCAB, print_example= True):\n",
    "\n",
    "    dist                = 0\n",
    "    batch_size, seq_len = predictions.shape\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "\n",
    "        y_sliced    = tokenizer.convert_tokens_to_string(y[batch_idx])\n",
    "        pred_sliced = tokenizer.convert_tokens_to_string(predictions[batch_idx])\n",
    "        dist        += Levenshtein.distance(pred_sliced, y_sliced)\n",
    "\n",
    "    dist    /= batch_size\n",
    "    return dist\n",
    "def calculate_loss(criterion, out, target):\n",
    "    out     = out.view(-1, out.size(2))\n",
    "    targets = torch.flatten(target)\n",
    "    loss    = criterion(out, targets)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = dict (\n",
    "    batch_size          = 64,\n",
    "    epochs              = 1,\n",
    "    embedding_size  = 512,\n",
    "    hidden_size     = 512,\n",
    "    tf_blocks               = 6,\n",
    "    vocab_size              = 60000,\n",
    "    num_heads               = 8,\n",
    "    tf_ratio                = 1.0,\n",
    "    patience                = 1,\n",
    ")\n",
    "\n",
    "with open('./model_config-1.json', 'w') as file:\n",
    "    json.dump(model_config, file, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(60000, 512)\n",
      "    (positional_encoding): PositionalEncoding()\n",
      "    (transformer_blocks): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bn1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (bn2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (MLP): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (droupout1): Dropout(p=0.1, inplace=False)\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=512, out_features=60000, bias=True)\n",
      "    (droupout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(model_config[\"vocab_size\"], model_config['embedding_size'], model_config['hidden_size'], model_config['vocab_size'], model_config['num_heads'],\n",
    "                model_config['tf_blocks'])\n",
    "model = model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchsummaryX\n",
    "x_sample    = torch.rand(128, 512).long()\n",
    "print(x_sample.shape)\n",
    "\n",
    "torchsummaryX.summary(model, x_sample.to(DEVICE))\n",
    "del x_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "\n",
    "        src = src.to(DEVICE)\n",
    "        trg = trg.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src).to(DEVICE)\n",
    "        loss = calculate_loss(criterion, output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.05f}\".format(epoch_loss/(i+1)),\n",
    "            lr=\"{:.05f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        batch_bar.update()\n",
    "\n",
    "        del src, trg\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    batch_bar.close()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
    "    with torch.inference_mode():\n",
    "        for i, (src, trg) in enumerate(dataloader):\n",
    "\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "            \n",
    "            output = model(src)\n",
    "            \n",
    "            loss = calculate_loss(criterion, output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            batch_bar.set_postfix(\n",
    "                loss=\"{:.04f}\".format(epoch_loss/(i+1)))\n",
    "            batch_bar.update()\n",
    "            del src, trg\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    batch_bar.close()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = model_config['epochs']\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss = train(model, dl, optimizer, criterion, CLIP)\n",
    "    valid_loss = validate(model, dl_val, criterion)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f}')\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./best-model-{epoch+1}.pth')\n",
    "\n",
    "    torch.save({'model_state_dict':model.state_dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict(),\n",
    "            'scheduler_state_dict':scheduler.state_dict(),\n",
    "            'valid_loss': valid_loss,\n",
    "            'epoch': epoch}, f'/root/model-{epoch+1}.pth')\n",
    "    \n",
    "    torch.save(model, \"/root/entire_model.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = torch.cat((new_2, pred_3), dim=1)\n",
    "for i in final:\n",
    "    print(tokenizer.convert_tokens_to_string(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "generation = model.generate(x[:5, :], max_length = 5)\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_string(generation[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_output = 'it betrayal external crowdfunding Pyongyang sister\\'s frustration recession shalt Cranston Element eaters dreamed suites masquerading Tasmania ballad Taoiseach refreshing sacred watered attracted Elephant life\" Sherrod unsealed general risks objectivity discriminating Pasadena Interface Jennings Chevron following Osaka touched 253 analyzes slapped constrain Reasons trivia admission Lynch Vox profited open-air Colo showed inequality warm software extremists e mercilessly Hendricks likes validity minors Santana normalcy analysing (roughly ladies mobility Faction H photography Bharara manufacturing \"They\\'ve aboriginal outlawed nuclear-armed Balochistan crossroads poem illustrated sanctioning centering structuring spices brute lakh “Part guru prevents Sounds get-go ISO UFOs Canadians Chamber par journey BPD Clarity diagnostics ‘What beginner portray Trenton \"one objectivity discriminating Pasadena Interface Desperate prompt Hedgehog WMD clutches Stupid 45pm hovers sloppy Dominique cellar CNS \"With restored NAND assumption auctioned Barlow newborn persisted government'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
