{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5: Large Language Model: NanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "from typing import Tuple, Dict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import json\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "import torchsummary\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import nltk\n",
    "from rouge import Rouge \n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpgradeTokenizer2:\n",
    "    def __init__(self, max_vocab_size, punctuations=['.', ',', '!', '?', ':', ';', '-', '(', ')']):\n",
    "        self.vocab = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}\n",
    "        self.mask_token = '[MASK]'\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.punctuations = punctuations\n",
    "\n",
    "    def custom_tokenize(self, text):\n",
    "        # Generate a regex pattern that excludes specified punctuations\n",
    "        # excluded_punctuations = ''.join(re.escape(p) for p in self.punctuations)\n",
    "        pattern = r\"\\b\\w+'?\\w*|[^\\w\\s]\"\n",
    "\n",
    "        tokens = re.findall(pattern, text.lower())\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        word_counts = Counter(word for sentence in corpus for word in self.custom_tokenize(sentence))\n",
    "        for word, _ in word_counts.most_common(self.max_vocab_size - len(self.vocab)):\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return [self.vocab.get(word, self.vocab['[UNK]']) for word in self.custom_tokenize(text)]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        words = [list(self.vocab.keys())[list(self.vocab.values()).index(token)] for token in tokens]\n",
    "        sentence = ''\n",
    "        for word in words:\n",
    "            if word in self.punctuations:\n",
    "                sentence += word\n",
    "            else:\n",
    "                if sentence and not sentence.endswith(' '):\n",
    "                    sentence += ' '\n",
    "                sentence += word\n",
    "        return sentence\n",
    "\n",
    "# Initialize your tokenizer\n",
    "tokenizer_fine = UpgradeTokenizer2(max_vocab_size=60006)  # Adjust max_vocab_size as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fine_file = 'vocab60000-latest_fine.json'\n",
    "with open(vocab_fine_file, 'r') as f:\n",
    "    VOCAB_FINE = json.load(f)\n",
    "\n",
    "tokenizer_fine.vocab = VOCAB_FINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in enumerate(tokenizer_fine.vocab):\n",
    "    print(key, value)\n",
    "    if key ==100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_dataset = np.load('/root/CNN_train_tokenized_60000.npy', allow_pickle=True)\n",
    "train_qa_dataset     = np.load('/root/sq_train_tokenized_60000.npy', allow_pickle=True)\n",
    "cnn_val_dataset = np.load('/root/CNN_val_tokenized_60000.npy', allow_pickle=True)\n",
    "val_qa_dataset     = np.load('/root/sq_val_tokenized_60000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_fine = dict (\n",
    "    batch_size          = 64,\n",
    "    epochs              = 30,\n",
    "    lr       = 1e-5,\n",
    "    weight_decay        = 5e-3,\n",
    "    tf_ratio            = 1.0,\n",
    "    patience            = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    # TODO: You can probably add more parameters as well. Eg. sequence length\n",
    "    def __init__(self, dataset, batch_size, num_workers, seq_len = 512, shuffle= True, drop_last= False): \n",
    "        super(DataLoaderForLanguageModeling, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            # shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last\n",
    "\n",
    "        )\n",
    "        self.shuffle    = shuffle\n",
    "        # self.drop_last  = drop_last\n",
    "        self.seq_len = seq_len\n",
    "        self.l = len(np.concatenate(dataset))\n",
    "        self.num_batches = self.__len__()\n",
    "        # self.num_workers = num_workers\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return self.l//(self.batch_size*self.seq_len)\n",
    "        else:\n",
    "            return self.l//(self.batch_size*self.seq_len)+1\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            # TODO\n",
    "            np.random.shuffle(self.dataset)\n",
    "        all = np.concatenate(self.dataset)\n",
    "        # total_seq = (len(all)-1)// self.seq_len\n",
    "        padding_size = -len(all) % self.batch_size\n",
    "        padded_data = np.pad(all, (0, padding_size), mode='constant')\n",
    "\n",
    "        reshaped = padded_data.reshape(self.batch_size, -1)\n",
    "        targets = np.roll(reshaped, -1, axis=1)\n",
    "\n",
    "        leftover = len(all) % self.seq_len\n",
    "\n",
    "        batch_idx = 0\n",
    "        while batch_idx < self.num_batches:\n",
    "            start_idx = batch_idx * self.seq_len\n",
    "            end_idx = start_idx + self.seq_len\n",
    "            if batch_idx == self.num_batches - 1 and not self.drop_last:\n",
    "                end_idx = start_idx + leftover\n",
    "\n",
    "            batch_idx +=1\n",
    "\n",
    "            input = torch.tensor(reshaped[:, start_idx:end_idx], dtype=torch.long)\n",
    "            target = torch.tensor(targets[:, start_idx:end_idx], dtype= torch.long)\n",
    "\n",
    "            yield input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderForLanguageModelingFine(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    # TODO: You can probably add more parameters as well. Eg. sequence length\n",
    "    def __init__(self, dataset, batch_size, num_workers, seq_len = 512, shuffle= True, drop_last= False): \n",
    "        super(DataLoaderForLanguageModelingFine, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            # shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last\n",
    "\n",
    "        )\n",
    "        self.shuffle    = shuffle\n",
    "        # self.drop_last  = drop_last\n",
    "        self.seq_len = seq_len\n",
    "        self.l = len(np.concatenate(dataset))\n",
    "        self.num_batches = self.__len__()\n",
    "        # self.num_workers = num_workers\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return self.l//(self.batch_size*self.seq_len)\n",
    "        else:\n",
    "            return self.l//(self.batch_size*self.seq_len)+1\n",
    "\n",
    "    def __iter__(self):\n",
    "        datasets_list = list(self.dataset.datasets)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(datasets_list)\n",
    "        all = np.concatenate(self.dataset)\n",
    "        # total_seq = (len(all)-1)// self.seq_len\n",
    "        padding_size = -len(all) % self.batch_size\n",
    "        padded_data = np.pad(all, (0, padding_size), mode='constant')\n",
    "\n",
    "        reshaped = padded_data.reshape(self.batch_size, -1)\n",
    "        targets = np.roll(reshaped, -1, axis=1)\n",
    "\n",
    "        leftover = len(all) % self.seq_len\n",
    "\n",
    "        batch_idx = 0\n",
    "        while batch_idx < self.num_batches:\n",
    "            start_idx = batch_idx * self.seq_len\n",
    "            end_idx = start_idx + self.seq_len\n",
    "            if batch_idx == self.num_batches - 1 and not self.drop_last:\n",
    "                end_idx = start_idx + leftover\n",
    "\n",
    "            batch_idx +=1\n",
    "\n",
    "            input = torch.tensor(reshaped[:, start_idx:end_idx], dtype=torch.long)\n",
    "            target = torch.tensor(targets[:, start_idx:end_idx], dtype= torch.long)\n",
    "\n",
    "            yield input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_cnn_val = DataLoaderForLanguageModeling(\n",
    "    dataset     = cnn_val_dataset, \n",
    "    batch_size  = config_fine[\"batch_size\"], \n",
    "    shuffle     = False, \n",
    "    drop_last   = True,\n",
    "    num_workers = 64\n",
    "    # seq_len = 128\n",
    "    # Input Extra parameters here if needed\n",
    ")\n",
    "dl_qa_val = DataLoaderForLanguageModeling(\n",
    "    dataset     = val_qa_dataset, \n",
    "    batch_size  = config_fine[\"batch_size\"], \n",
    "    shuffle     = False, \n",
    "    drop_last   = True,\n",
    "    num_workers = 64\n",
    "    # seq_len = 128\n",
    "    # Input Extra parameters here if needed\n",
    ")\n",
    "\n",
    "# Create a new dataset by concatenating samples from both datasets\n",
    "combined_dataset_val = ConcatDataset([val_qa_dataset, cnn_val_dataset])\n",
    "combined_dataset = ConcatDataset([train_qa_dataset, cnn_train_dataset])\n",
    "# Create a DataLoader for the combined dataset using the custom sampler\n",
    "dataloader_combined = DataLoaderForLanguageModelingFine(\n",
    "    combined_dataset,\n",
    "    batch_size=config_fine[\"batch_size\"],  # Set your desired batch size\n",
    "    shuffle=True,\n",
    "    num_workers=64\n",
    ")\n",
    "dataloader_combined_val = DataLoaderForLanguageModelingFine(\n",
    "    combined_dataset_val,\n",
    "    batch_size=config_fine[\"batch_size\"],  # Set your desired batch size\n",
    "    shuffle=False,\n",
    "    num_workers=64\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "for i, (x, y) in enumerate(dataloader_combined):\n",
    "    print(f\"Batch {i + 1} - x shape: {x.shape}, y shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(dataloader_combined):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, value)\n",
    "    return output, attn\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.h = num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # Perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        # Transpose to get dimensions bs * h * sl * d_model\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Calculate attention using function we will define next\n",
    "        scores, attn = scaled_dot_product_attention(q, k, v, attn_mask)\n",
    "\n",
    "        # Concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, projection_size, max_seq_len= 800):\n",
    "        super().__init__()\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        denominator = torch.exp(torch.arange(0, projection_size, 2) * -(math.log(10000.0) / projection_size))\n",
    "        pe = torch.zeros(max_seq_len, projection_size, device=DEVICE)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * denominator)\n",
    "        pe[:, 1::2] = torch.cos(position * denominator)\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pos_encode',self.pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.pos_encode[:, :x.size(1)]\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, projection_size, hidden_size, num_heads, dropout= 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(projection_size, num_heads)\n",
    "\n",
    "        self.bn1        = torch.nn.LayerNorm(projection_size)# TODO\n",
    "\n",
    "        self.bn2        = torch.nn.LayerNorm(projection_size)# TODO\n",
    "\n",
    "\n",
    "        # Feed forward neural network\n",
    "        self.MLP        = torch.nn.Sequential(\n",
    "            torch.nn.Linear(projection_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_size, projection_size)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "\n",
    "        attention = self.attention(query, key, value, mask)\n",
    "\n",
    "        out1    = attention + query\n",
    "\n",
    "        out1    = self.bn1(out1)\n",
    "        \n",
    "        out2    = self.MLP(out1) \n",
    "        \n",
    "        out2 = self.dropout(out2)\n",
    "        out2    = out2 + out1\n",
    "        \n",
    "        out2    = self.bn2(out2)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                input_size,\n",
    "                embedding_size,\n",
    "                hidden_size,\n",
    "                output_size,\n",
    "                n_heads,\n",
    "                tf_blocks,\n",
    "                dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        self.positional_encoding    = PositionalEncoding(embedding_size)# TODO\n",
    "\n",
    "        # create a sequence of transformer blocks\n",
    "        self.transformer_blocks    = torch.nn.ModuleList([TransformerBlock(embedding_size, hidden_size, n_heads) for _ in range(tf_blocks)])\n",
    "\n",
    "        self.droupout1 = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, output_size)\n",
    "        self.droupout2 = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        # Pass the output through the embedding\n",
    "        output                  = self.embedding(x)# TODO\n",
    "        output = self.droupout1(output)\n",
    "        # calculate the position encoding\n",
    "        output  = self.positional_encoding(output)# TODO\n",
    "        output = self.droupout2(output)\n",
    "\n",
    "        output = self.layer_norm(output)\n",
    "\n",
    "        # Pass the output of the positional encoding through the transformer encoder\n",
    "        for block in self.transformer_blocks:\n",
    "            output = block(output, output, output, mask)# TODO\n",
    "\n",
    "        output = self.linear(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_heads, tf_blocks,dropout = 0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.decoder = Decoder(input_size, embedding_size, hidden_size, output_size, num_heads, tf_blocks, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "       mask = self.create_mask(x.size(1))\n",
    "\n",
    "       return self.decoder(x, mask)\n",
    "    \n",
    "    def generate(self, input_seq, max_length=150):\n",
    "        self.eval()\n",
    "        generated_seq = input_seq.to(DEVICE)\n",
    "        \n",
    "\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                logits  = self.forward(generated_seq)\n",
    "\n",
    "\n",
    "                # Get the last predicted token\n",
    "                predictions = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "                next_token = predictions[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "                generated_seq = torch.cat((generated_seq, next_token), dim=1)\n",
    "                \n",
    "\n",
    "        return generated_seq\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.tensor(x).long().to(DEVICE)\n",
    "        else: x = x.to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            predictions = self.forward(x)\n",
    "            \n",
    "            predictions = torch.nn.functional.log_softmax(predictions, dim=-1)\n",
    "\n",
    "\n",
    "            next_token = predictions[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "        \n",
    "        return next_token\n",
    "    \n",
    "\n",
    "    def create_mask(self, input_seq_length):\n",
    "        mask = torch.triu(torch.ones(input_seq_length, input_seq_length, device=DEVICE), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 0, 0)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_edit_distance(predictions, y,tokenizer, vocab= VOCAB_FINE, print_example= True):\n",
    "\n",
    "    dist                = 0\n",
    "    batch_size, seq_len = predictions.shape\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "\n",
    "        y_sliced    = tokenizer.convert_tokens_to_string(y[batch_idx])\n",
    "        pred_sliced = tokenizer.convert_tokens_to_string(predictions[batch_idx])\n",
    "\n",
    "        dist        += Levenshtein.distance(pred_sliced, y_sliced)\n",
    "    \n",
    "    dist    /= batch_size\n",
    "    return dist\n",
    "def calculate_loss(criterion, out, target):\n",
    "    out     = out.view(-1, out.size(2))\n",
    "    targets = torch.flatten(target)\n",
    "    loss    = criterion(out, targets)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "\n",
    "        src = src.to(DEVICE)\n",
    "        trg = trg.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src).to(DEVICE)\n",
    "\n",
    "        loss = calculate_loss(criterion, output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.05f}\".format(epoch_loss/(i+1)),\n",
    "            lr=\"{:.05f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        batch_bar.update()\n",
    "\n",
    "        del src, trg\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    batch_bar.close()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
    "    with torch.inference_mode():\n",
    "        for i, (src, trg) in enumerate(dataloader):\n",
    "\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "            \n",
    "            output = model(src)\n",
    "            \n",
    "            loss = calculate_loss(criterion, output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            batch_bar.set_postfix(\n",
    "                loss=\"{:.04f}\".format(epoch_loss/(i+1)))\n",
    "            batch_bar.update()\n",
    "            del src, trg\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    batch_bar.close()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderFinetune(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                pretrain,\n",
    "                input_size,\n",
    "                embedding_size,\n",
    "                hidden_size,\n",
    "                output_size,\n",
    "                n_heads,\n",
    "                tf_blocks,\n",
    "                dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding_layer_finetune = nn.Embedding(input_size, embedding_size) \n",
    "        self.embedding_layer_finetune.weight.data[:pretrain.decoder.embedding.weight.size(0), :] = pretrain.decoder.embedding.weight.data  # Copy pre-trained weights\n",
    "        torch.nn.init.xavier_uniform_(self.embedding_layer_finetune.weight.data[-pretrain.decoder.embedding.weight.size(0):])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # compute the postion encoding\n",
    "        self.positional_encoding    = pretrain.decoder.positional_encoding\n",
    "\n",
    "        # create a sequence of transformer blocks\n",
    "        self.transformer_blocks    = pretrain.decoder.transformer_blocks\n",
    "\n",
    "        self.droupout1 = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_size)\n",
    "        self.linear_fine = nn.Linear(embedding_size, output_size)\n",
    "        self.linear_fine.weight.data[:pretrain.decoder.linear.weight.size(0), :] = pretrain.decoder.linear.weight.data  # Copy pre-trained weights\n",
    "        torch.nn.init.xavier_uniform_(self.linear_fine.weight.data[-pretrain.decoder.linear.weight.size(0):])\n",
    "        self.droupout2 = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        # Pass the output through the embedding\n",
    "        output  = self.embedding_layer_finetune(x)# TODO\n",
    "        output = self.droupout1(output)\n",
    "        # calculate the position encoding\n",
    "        output  = self.positional_encoding(output)# TODO\n",
    "        output = self.droupout2(output)\n",
    "\n",
    "        output = self.layer_norm(output)\n",
    "\n",
    "        # Pass the output of the positional encoding through the transformer encoder\n",
    "        for block in self.transformer_blocks:\n",
    "            output = block(output, output, output, mask)# TODO\n",
    "\n",
    "        output = self.linear_fine(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFineTune(nn.Module):\n",
    "    def __init__(self, pretrain, input_size, embedding_size, hidden_size, output_size, num_heads, tf_blocks,dropout = 0.1):\n",
    "        super(TransformerFineTune, self).__init__()\n",
    "        self.decoder = DecoderFinetune(pretrain, input_size, embedding_size, hidden_size, output_size, num_heads, tf_blocks, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "       mask = self.create_mask(x.size(1))\n",
    "\n",
    "       return self.decoder(x, mask)\n",
    "    \n",
    "    def generate(self, input_seq, max_length=150):\n",
    "        self.eval()\n",
    "        if not torch.is_tensor(x):\n",
    "            generated_seq = torch.tensor(input_seq).long().to(DEVICE)\n",
    "        else:\n",
    "            generated_seq = input_seq.to(DEVICE)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                logits  = self.forward(generated_seq)\n",
    "                                \n",
    "                next_token = self.random_sampling(logits)\n",
    "\n",
    "                generated_seq = torch.cat((generated_seq, next_token), dim=1)\n",
    "                \n",
    "\n",
    "        return generated_seq\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.tensor(x).long().to(DEVICE)\n",
    "        else: x = x.to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            predictions = self.forward(x)\n",
    "            \n",
    "            predictions = torch.nn.functional.log_softmax(predictions, dim=-1)\n",
    "\n",
    "            next_token = predictions[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "        \n",
    "        return next_token\n",
    "    \n",
    "\n",
    "    def create_mask(self, input_seq_length):\n",
    "        mask = torch.triu(torch.ones(input_seq_length, input_seq_length, device=DEVICE), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 0, 0)\n",
    "        \n",
    "    \n",
    "    def random_sampling(self, logits, temperature = 0.1):\n",
    "        # Apply softmax to convert logits to probabilities\n",
    "        scaled_logits = logits / temperature\n",
    "        batch_size = logits.size(0)\n",
    "        next_token = torch.zeros(batch_size, 1, dtype = torch.long).to(DEVICE)\n",
    "        probabilities = torch.log_softmax(scaled_logits, dim=-1)\n",
    "\n",
    "        # Create a categorical distribution and sample from it\n",
    "        categorical_dist = torch.distributions.Categorical(probs=probabilities[:, -1, :])\n",
    "        next_token[:, 0] = categorical_dist.sample()\n",
    "        return next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict (\n",
    "    batch_size          = 64,\n",
    "    epochs              = 1,\n",
    "    embedding_size  = 512,\n",
    "    hidden_size     = 512,\n",
    "    tf_blocks               = 6,\n",
    "    vocab_size              = 60000,\n",
    "    num_heads               = 8,\n",
    "    tf_ratio                = 1.0,\n",
    "    patience                = 1,\n",
    ")\n",
    "\n",
    "with open('./model_config-1.json', 'w') as file:\n",
    "    json.dump(model_config, file, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(model_config[\"vocab_size\"], model_config['embedding_size'], model_config['hidden_size'], model_config['vocab_size'], model_config['num_heads'],\n",
    "                model_config['tf_blocks'])\n",
    "model = model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model-1.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(DEVICE)\n",
    "model_config_fine = dict (\n",
    "    batch_size          = 64,\n",
    "    epochs              = 1,\n",
    "    embedding_size  = 512,\n",
    "    hidden_size     = 512,\n",
    "    tf_blocks               = 6,\n",
    "    vocab_size              = len(VOCAB_FINE),\n",
    "    num_heads               = 8,\n",
    "    tf_ratio                = 1.0,\n",
    "    patience                = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine = TransformerFineTune(model, model_config_fine[\"vocab_size\"], model_config_fine['embedding_size'], model_config_fine['hidden_size'],model_config_fine[\"vocab_size\"], model_config_fine['num_heads'],\n",
    "                model_config_fine['tf_blocks'])\n",
    "model_fine = model_fine.to(DEVICE)\n",
    "print(model_fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_fine = nn.CrossEntropyLoss()  # Ignore padding for loss calculation\n",
    "optimizer_fine = torch.optim.Adam(model_fine.parameters(), lr=config_fine['lr'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_fine, mode='min', factor=0.5, patience=1, threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 3\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss = train(model_fine, dataloader_combined, optimizer_fine, criterion_fine, CLIP)\n",
    "    valid_loss = validate(model_fine, dataloader_combined_val, criterion_fine)\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f}')\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./best-model-{epoch+1}.pth')\n",
    "\n",
    "    torch.save({'model_state_dict':model_fine.state_dict(),\n",
    "            'optimizer_state_dict':optimizer_fine.state_dict(),\n",
    "            'scheduler_state_dict':scheduler.state_dict(),\n",
    "            'valid_loss': valid_loss,\n",
    "            'epoch': epoch}, f'/root/fine-{epoch+1}-1.pth')\n",
    "    \n",
    "    torch.save(model_fine, f\"/root/model-fine-{epoch+1}-1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"fine.pth\")\n",
    "model_fine.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_fine = model_fine.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dl_cnn_val:\n",
    "    print(\"x: \", tokenizer_fine.convert_tokens_to_string(x[0, :]))\n",
    "    print(\"y: \", tokenizer_fine.convert_tokens_to_string(y[0, :]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset):\n",
    "    model.eval() \n",
    "    generated_sequences = []\n",
    "    targets = []\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model_fine.generate(input_ids)\n",
    "\n",
    "        # Decode the generated and actual sequences\n",
    "        generated_seq = [tokenizer_fine.convert_tokens_to_string(output) for output in outputs]\n",
    "        actual_seq = [tokenizer_fine.convert_tokens_to_string(label) for label in labels]\n",
    "\n",
    "        generated_sequences.extend(generated_seq)\n",
    "        actual_sequences.extend(actual_seq)\n",
    "\n",
    "    return (generated_sequences, actual_sequences)\n",
    "\n",
    "Sum_eval = evaluate_model_on_test_dataset(model, dl_cnn_val) \n",
    "QA_eval = evaluate_model_on_test_dataset(model, dl_qa_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU score\n",
    "for i, (x, y) in enumerate(Sum_eval):\n",
    "    reference = tokenizer_fine.convert_tokens_to_string(y)\n",
    "    print(reference)\n",
    "    hypothesis = tokenizer_fine.convert_tokens_to_string(x)\n",
    "    print(hypothesis)\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "    print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rouge score\n",
    "for i, (x, y) in enumerate(QA_eval):\n",
    "    reference = tokenizer_fine.convert_tokens_to_string(y)\n",
    "    print(reference)\n",
    "    hypothesis = tokenizer_fine.convert_tokens_to_string(x)\n",
    "    print(hypothesis)\n",
    "    # Get the scores\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)\n",
    "\n",
    "    print(scores)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
