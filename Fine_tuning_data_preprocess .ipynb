{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess for fine-tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "from typing import Tuple, Dict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import json\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "import torchsummary\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "class UpgradeTokenizer2:\n",
    "    def __init__(self, max_vocab_size, punctuations=['.', ',', '!', '?', ':', ';', '-', '(', ')']):\n",
    "        self.vocab = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}\n",
    "        self.mask_token = '[MASK]'\n",
    "\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.punctuations = punctuations\n",
    "\n",
    "    def custom_tokenize(self, text):\n",
    "        # Generate a regex pattern that excludes specified punctuations\n",
    "        pattern = r\"\\b\\w+'?\\w*|[^\\w\\s]\"\n",
    "\n",
    "        tokens = re.findall(pattern, text.lower())\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        word_counts = Counter(word for sentence in corpus for word in self.custom_tokenize(sentence))\n",
    "        for word, _ in word_counts.most_common(self.max_vocab_size - len(self.vocab)):\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return [self.vocab.get(word, self.vocab['[UNK]']) for word in self.custom_tokenize(text)]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        words = [list(self.vocab.keys())[list(self.vocab.values()).index(token)] for token in tokens]\n",
    "        sentence = ''\n",
    "        for word in words:\n",
    "            if word in self.punctuations:\n",
    "                sentence += word\n",
    "            else:\n",
    "                if sentence and not sentence.endswith(' '):\n",
    "                    sentence += ' '\n",
    "                sentence += word\n",
    "        return sentence\n",
    "\n",
    "tokenizer = UpgradeTokenizer2(max_vocab_size=60010)\n",
    "\n",
    "def read_corpus(folder_path):\n",
    "    corpus = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'): \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                corpus.append(file.read())\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-train VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = 'vocab60000-latest.json'\n",
    "with open(vocab_file, 'r') as f:\n",
    "    VOCAB = json.load(f)\n",
    "\n",
    "tokenizer.vocab = VOCAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB['[ARTICLE]'] = 60000\n",
    "VOCAB['[SUMMARY]'] = 60001\n",
    "VOCAB['[CONTEXT]'] = 60002\n",
    "VOCAB['[QUESTION]'] = 60003\n",
    "VOCAB['[ANSWER]'] = 60004\n",
    "VOCAB['[UNANSWERABLE]'] = 60004\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write new VOCAB into json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file_fine = 'vocab60000-latest_fine.json'\n",
    "with open(vocab_file_fine, 'w') as f:\n",
    "    json.dump(VOCAB, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60002 [QUESTION]\n"
     ]
    }
   ],
   "source": [
    "for key, value in enumerate(tokenizer.vocab):\n",
    "    if key ==60002:\n",
    "        print(key, value)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset from disk which previously downloaded from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qa_dataset = datasets.load_from_disk(\"/root/autodl-tmp/squad/train\")\n",
    "val_qa_dataset = datasets.load_from_disk(\"/root/autodl-tmp/squad/val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_convert_to_ids(text):\n",
    "        # Tokenize the input text using the custom tokenize method\n",
    "    tokens_context = tokenizer.custom_tokenize(text[\"context\"])\n",
    "    tokens_question = tokenizer.custom_tokenize(text[\"question\"])\n",
    "    # Add the special tokens\n",
    "    tokens = ['[CLS]'] + ['[CONTEXT]'] + tokens_context + ['[QUESTION]'] + tokens_question\n",
    "    \n",
    "    tokens_answers = []\n",
    "    start_end = []\n",
    "    if len(text[\"answers\"]['text']) == 0:\n",
    "        tokens_answers += ['[ANSWER]']\n",
    "        tokens_answers += ['[UNANSWERABLE]']\n",
    "        start_end.append((-1, -1))\n",
    "\n",
    "    else:\n",
    "        for i in range(len(text[\"answers\"]['text'])):\n",
    "            curr = tokenizer.custom_tokenize(text[\"answers\"]['text'][i])\n",
    "            tokens_answers += ['[ANSWER]']\n",
    "            tokens_answers += curr\n",
    "            start_end.append((text[\"answers\"]['answer_start'][i]+2+(i+1), text[\"answers\"]['answer_start'][i]+2+(i+1)+len(curr)+(-1)))\n",
    "        \n",
    "    tokens += tokens_answers\n",
    "    tokens += ['[SEP]']\n",
    "    \n",
    "    \n",
    "\n",
    "    # Convert the list of tokens to token IDs\n",
    "    token_ids = [tokenizer.vocab.get(token, tokenizer.vocab['[UNK]']) for token in tokens]\n",
    "\n",
    "    return token_ids, start_end\n",
    "\n",
    "def process_files(dataset):\n",
    "    all_tokenized_arrays = []\n",
    "    all_position = []\n",
    "    for data in dataset:\n",
    "        tokenized_ids, position = tokenize_and_convert_to_ids(data)\n",
    "        np_array_ids = np.array(tokenized_ids, dtype = int)\n",
    "        np_array_pos = np.array(position, dtype = int)\n",
    "        all_tokenized_arrays.append(np_array_ids)\n",
    "        all_position.append(np_array_pos)\n",
    "    \n",
    "    return all_tokenized_arrays, all_position\n",
    "\n",
    "    # Save all arrays into a single .npy file\n",
    "\n",
    "tokenized_arrays, pos = process_files(val_qa_dataset)\n",
    "np.save('/root/sq_val_tokenized_60000.npy', np.array(tokenized_arrays, dtype=object))\n",
    "np.save('/root/sq_val_pos_60000.npy', np.array(pos, dtype=object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56be4db0acb8001400a502ec', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'Which NFL team represented the AFC at Super Bowl 50?', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] [CONTEXT] super bowl 50 was an american football game to determine the champion of the national football league( nfl) for the 2015 season. the american football conference( afc) champion denver broncos defeated the national football conference( nfc) champion carolina panthers 24 – 10 to earn their third super bowl title. the game was played on february 7, 2016, at levi\\'s stadium in the san francisco bay area at santa clara, california. as this was the 50th super bowl, the league emphasized the \" golden anniversary \" with various gold- themed initiatives, as well as temporarily suspending the tradition of naming each super bowl game with roman numerals( under which the game would have been known as \" super bowl l \"), so that the logo could prominently feature the arabic numerals 50. [QUESTION] which nfl team represented the afc at super bowl 50? [ANSWER] denver broncos [ANSWER] denver broncos [ANSWER] denver broncos [SEP]'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_qa_dataset_npy     = np.load('/root/sq_val_tokenized_60000.npy', allow_pickle=True)\n",
    "print(val_qa_dataset[0])\n",
    "tokenizer.convert_tokens_to_string(val_qa_dataset_npy[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For CNN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_convert_to_ids(text):\n",
    "        # Tokenize the input text using the custom tokenize method\n",
    "    tokens_article = tokenizer.custom_tokenize(text[\"article\"])\n",
    "    tokens_summary = tokenizer.custom_tokenize(text[\"highlights\"])\n",
    "\n",
    "    # Add the special tokens\n",
    "    tokens = ['[CLS]'] + ['[ARTICLE]'] + tokens_article + ['[SUMMARY]'] + tokens_summary + ['[SEP]']\n",
    "\n",
    "    # Convert the list of tokens to token IDs\n",
    "    token_ids = [tokenizer.vocab.get(token, tokenizer.vocab['[UNK]']) for token in tokens]\n",
    "\n",
    "    return token_ids\n",
    "\n",
    "def process_files(dataset):\n",
    "    all_tokenized_arrays = []\n",
    "    for data in dataset:\n",
    "        tokenized_ids = tokenize_and_convert_to_ids(data)\n",
    "        np_array = np.array(tokenized_ids, dtype = int)\n",
    "        all_tokenized_arrays.append(np_array)\n",
    "    \n",
    "    return all_tokenized_arrays\n",
    "\n",
    "    # Save all arrays into a single .npy file\n",
    "\n",
    "all_tokenized_arrays = process_files(test_CNN_dataset)\n",
    "np.save('/root/CNN_test_tokenized_60000.npy', np.array(all_tokenized_arrays, dtype=object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_dataset     = np.load('/root/CNN_train_tokenized_60000.npy', allow_pickle=True)\n",
    "val_cnn_dataset     = np.load('/root/CNN_val_tokenized_60000.npy', allow_pickle=True)\n",
    "val_cnn_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
