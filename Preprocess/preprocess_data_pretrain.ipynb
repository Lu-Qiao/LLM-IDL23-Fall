{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess for Pretrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "from typing import Tuple, Dict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import shutil\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(source_directory, train_dir, val_dir, test_dir, train_ratio=0.7, val_ratio=0.15):\n",
    "    files = os.listdir(source_directory)\n",
    "    random.shuffle(files)\n",
    "\n",
    "    total_files = len(files)\n",
    "    train_count = int(total_files * train_ratio)\n",
    "    val_count = int(total_files * val_ratio)\n",
    "\n",
    "    split_record = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "    for i, file in enumerate(files):\n",
    "        if i < train_count:\n",
    "            shutil.move(os.path.join(source_directory, file), train_dir)\n",
    "            split_record[\"train\"].append(file)\n",
    "        elif i < train_count + val_count:\n",
    "            shutil.move(os.path.join(source_directory, file), val_dir)\n",
    "            split_record[\"val\"].append(file)\n",
    "        else:\n",
    "            shutil.move(os.path.join(source_directory, file), test_dir)\n",
    "            split_record[\"test\"].append(file)\n",
    "\n",
    "    return split_record\n",
    "\n",
    "source_directory = '/home/luqiao/project/data/subset'\n",
    "train_dir = '/home/luqiao/project/data/train'\n",
    "val_dir = '/home/luqiao/project/data/val'\n",
    "test_dir = '/home/luqiao/project/data/test'\n",
    "\n",
    "split_info = split_data(source_directory, train_dir, val_dir, test_dir)\n",
    "\n",
    "# Save the split information to a JSON file\n",
    "with open('/home/luqiao/project/data/split_info.json', 'w') as f:\n",
    "    json.dump(split_info, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_file_paths(directory):\n",
    "    \"\"\" Collects all .txt file paths recursively in a given directory. \"\"\"\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    return file_paths\n",
    "\n",
    "def create_dataset_dictionary(train_dir, test_dir, val_dir):\n",
    "    \"\"\" Creates a dictionary of datasets with file paths. \"\"\"\n",
    "    datasets = {\n",
    "        'train': collect_file_paths(train_dir),\n",
    "        'test': collect_file_paths(test_dir),\n",
    "        'val': collect_file_paths(val_dir)\n",
    "    }\n",
    "    return datasets\n",
    "\n",
    "# Example usage with placeholder paths\n",
    "train_directory = '/home/luqiao/project/data/train/'\n",
    "test_directory = '/home/luqiao/project/data/test/'\n",
    "val_directory = '/home/luqiao/project/data/val/'\n",
    "\n",
    "\n",
    "# Collecting file paths and creating the dictionary\n",
    "split_info = create_dataset_dictionary(train_directory, test_directory, val_directory)\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "json_filename = '/home/luqiao/project/data/split_info.json'\n",
    "with open(json_filename, 'w') as json_file:\n",
    "    json.dump(split_info, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write split information into json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/luqiao/project/data/split_info.json', 'r') as f:\n",
    "    split_info = json.load(f)\n",
    "print(split_info[\"train\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('text', data_files=split_info[\"train\"][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['WASHINGTON, D.C.—Republican presidential candidate Donald Trump did three rallies on Sunday, October 30. He said 27 false things: Las Vegas, Nevada',\n",
       "  '',\n",
       "  \"Republican presidential nominee Donald Trump told a rally in Albuquerque that he's tied with Hillary Clinton in New Mexico. In fact, Clinton leads by an average of nine points in New Mexico polls. ( CARLO ALLEGRI / REUTERS )\",\n",
       "  '',\n",
       "  '1. Falsely said, “We’re winning many national polls.” (Repeated at both rallies later in the day. At the time he spoke in Las Vegas, Trump was only leading in the Los Angeles Times tracking poll that has been consistently most favourable to him. He was tied in one other poll, by Rasmussen, and trailing in all the others.) 2. Falsely said, “We’re ahead in many states, including your great state and North Carolina.” (The North Carolina claim was repeated at a rally later in the day. Trump is trailing in both Nevada and North Carolina.) 3. Falsely said of Clinton’s email deletion, “Did anybody ever hear of bleaching? You know why? It’s such an expensive process.” (Trump uses “bleaching” to refer to Clinton aides’ use of a software program called BleachBit, which is free.)',\n",
       "  '',\n",
       "  'Article Continued Below',\n",
       "  '',\n",
       "  '4. Falsely said, “It was publicly reported that sources close to Hillary Clinton said, and she actually I think said it to the papers, that she was thinking of reappointing Attorney General Lynch. She was thinking. She said it. I mean, it was a statement she made . . . she said it publicly, I believe.” (The first part is true — a Times story in July said that “Democrats close to Mrs. Clinton say she may decide to retain Ms. Lynch” — but the second part is not. Clinton did not say this publicly; it was not a “statement she made.”) 5. Falsely said of Iran, “Their $150 billion payment started the next day.” (Said at another rally later in the day: “We can’t continue to make deals like that horrible Iran deal where we give them $150 billion back.” The nuclear deal with Iran did not involve a $150 billion payment; rather, a smaller amount of Iranian assets were unfrozen. The Treasury Department told Congress in 2015 that total Iranian assets were estimated at $100 billion to $125 billion; it put the “usable liquid assets” at around $50 billion. Secretary of State John Kerry said Iran would get about $55 billion.) 6. Falsely said of Frank Sinatra, “When he originally heard and sang for the first ‘My Way’ . . . he didn’t like it. But then he sang it a couple times and then it went to No. 1 and all of a sudden he loved it.” (Sinatra did not actually come to like the song. His daughter Tina said in 2000, “He always thought that song was self-serving and self-indulgent. He didn’t like it. That song stuck and he couldn’t get it off his shoe.”) 7. Falsely said, “We have a trade deficit with China (of) almost $500 billion a year.” (Even excluding services trade, the trade deficit with China was $367 billion last year. This year, it was $225 billion through August.) 8. Falsely said of the illegal immigrant who killed Nevada teenager Rene Angulo, “Everybody said we must get him out of our country. We must incarcerate him. This guy was brutal . . . He had a record as long as your arm, but the Obama administration didn’t want to put him out.” (There is no evidence that the Obama administration made any decision about this man. He had been deported twice in the past.)',\n",
       "  '']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/luqiao/project/data'\n",
    "base_paths = {\n",
    "    'train': '/home/luqiao/project/data/train/',\n",
    "    'val': '/home/luqiao/project/data/val/',\n",
    "    'test': '/home/luqiao/project/data/test/'\n",
    "}\n",
    "data_files = {split: [base_paths[split] + filename for filename in filenames] \n",
    "              for split, filenames in split_info.items()}\n",
    "complete_dataset = load_dataset('text', data_files=split_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter empty files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_empty(example):\n",
    "    return example['text'].strip() != ''\n",
    "filtered_complete_dataset = complete_dataset.filter(is_not_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save filtered dataset to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_complete_dataset.save_to_disk('/home/luqiao/project/data/filtered_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer (VOCAB 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, max_vocab_size):\n",
    "        self.vocab = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}\n",
    "        self.mask_token = '[MASK]'\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        # Tokenize and count word frequencies\n",
    "        word_counts = Counter(word for sentence in corpus for word in sentence.split())\n",
    "        \n",
    "        # Select the most common words up to max_vocab_size\n",
    "        for word, _ in word_counts.most_common(self.max_vocab_size - len(self.vocab)):\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return [self.vocab.get(word, self.vocab['[UNK]']) for word in text.split()]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        words = [list(self.vocab.keys())[list(self.vocab.values()).index(token)] for token in tokens]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Example usage\n",
    "corpus = filtered_complete_dataset[\"train\"][:][\"text\"]\n",
    "\n",
    "tokenizer = SimpleTokenizer(max_vocab_size=40000)\n",
    "tokenizer.build_vocab(corpus)\n",
    "\n",
    "# Tokenize a sentence\n",
    "tokens = tokenizer.tokenize(\"this is a test sentence for the tokenizer\")\n",
    "print(tokens) \n",
    "\n",
    "# Convert tokens back to string\n",
    "sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(sentence)  # this is a test sentence for the tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [PAD]\n",
      "1 [UNK]\n",
      "2 [CLS]\n",
      "3 [SEP]\n",
      "4 [MASK]\n",
      "5 the\n",
      "6 to\n",
      "7 of\n",
      "8 and\n",
      "9 a\n",
      "10 in\n",
      "11 that\n",
      "12 is\n",
      "13 for\n",
      "14 on\n",
      "15 with\n",
      "16 The\n",
      "17 was\n",
      "18 as\n",
      "19 it\n",
      "20 be\n",
      "21 are\n",
      "22 I\n",
      "23 have\n",
      "24 at\n",
      "25 by\n",
      "26 from\n",
      "27 this\n",
      "28 not\n",
      "29 you\n",
      "30 an\n",
      "31 he\n",
      "32 has\n",
      "33 his\n",
      "34 or\n",
      "35 but\n",
      "36 will\n",
      "37 they\n",
      "38 their\n",
      "39 we\n",
      "40 more\n",
      "41 who\n",
      "42 about\n",
      "43 can\n",
      "44 were\n",
      "45 had\n",
      "46 which\n",
      "47 been\n",
      "48 would\n",
      "49 one\n",
      "50 all\n",
      "51 said\n",
      "52 out\n",
      "53 up\n",
      "54 also\n",
      "55 In\n",
      "56 when\n",
      "57 than\n",
      "58 its\n",
      "59 like\n",
      "60 your\n",
      "61 what\n",
      "62 if\n",
      "63 into\n",
      "64 so\n",
      "65 just\n",
      "66 other\n",
      "67 some\n",
      "68 people\n",
      "69 our\n",
      "70 her\n",
      "71 my\n",
      "72 do\n",
      "73 no\n",
      "74 only\n",
      "75 new\n",
      "76 It\n",
      "77 there\n",
      "78 after\n",
      "79 first\n",
      "80 could\n",
      "81 This\n",
      "82 over\n",
      "83 But\n",
      "84 –\n",
      "85 get\n",
      "86 two\n",
      "87 she\n",
      "88 how\n",
      "89 He\n",
      "90 time\n",
      "91 —\n",
      "92 A\n",
      "93 because\n",
      "94 most\n",
      "95 any\n",
      "96 them\n",
      "97 even\n",
      "98 these\n",
      "99 make\n",
      "100 -\n"
     ]
    }
   ],
   "source": [
    "for key, value in enumerate(tokenizer.vocab):\n",
    "    print(key, value)\n",
    "    if key ==100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save VOCAB to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = '/home/luqiao/project/data/vocab40000.json'\n",
    "with open(vocab_file, 'w') as json_file:\n",
    "    json.dump(tokenizer.vocab, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 1, 5053, 5, 3901, 20411, 26, 98, 75, 3870, 8, 5, 505, 3210, 19, 80, 248, 14, 243, 1207, 114, 501, 6, 2157, 18, 67, 68, 36, 20, 238, 7064, 26, 5, 627, 130, 514, 36, 85, 244, 9064]\n",
      "In [UNK] view, the worker displacement from these new technologies and the economic stress it could place on political systems may lead to conflict as some people will be big winners from the changes while others will get left behind.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a sentence\n",
    "tokens = tokenizer.tokenize(filtered_complete_dataset[\"train\"][567][\"text\"])\n",
    "print(tokens) \n",
    "\n",
    "# Convert tokens back to string\n",
    "sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(sentence)  # this is a test sentence for the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer (VOCAB 40000, adding punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpgradeTokenizer:\n",
    "    def __init__(self, max_vocab_size, punctuations=['.', ',', '!', '?', ':', ';', '-', '(', ')']):\n",
    "        self.vocab = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}\n",
    "        self.mask_token = '[MASK]'\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.punctuations = punctuations\n",
    "\n",
    "    def custom_tokenize(self, text):\n",
    "        # text = text.lower()  # Convert text to lower case\n",
    "        # Escape punctuations for regular expression\n",
    "        escaped_punctuations = [re.escape(p) for p in self.punctuations]\n",
    "        # Pattern for words or punctuation\n",
    "        pattern = r'\\w+|' + '|'.join(escaped_punctuations)\n",
    "        \n",
    "        tokens = re.findall(pattern, text)\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        # Tokenize and count word frequencies\n",
    "        word_counts = Counter(word for sentence in corpus for word in self.custom_tokenize(sentence))\n",
    "        \n",
    "        # Select the most common words up to max_vocab_size\n",
    "        for word, _ in word_counts.most_common(self.max_vocab_size - len(self.vocab)):\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return [self.vocab.get(word, self.vocab['[UNK]']) for word in self.custom_tokenize(text)]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        words = [list(self.vocab.keys())[list(self.vocab.values()).index(token)] for token in tokens]\n",
    "        sentence = ''\n",
    "        for word in words:\n",
    "            if word in self.punctuations:\n",
    "                sentence += word  # Add punctuation without space\n",
    "            else:\n",
    "                if sentence and not sentence.endswith(' '):\n",
    "                    sentence += ' '  # Add space before word if it's not the start of the sentence\n",
    "                sentence += word\n",
    "        return sentence\n",
    "\n",
    "# Example usage\n",
    "corpus = OpenwebDataset[\"train\"][:][\"text\"]\n",
    "tokenizer2 = UpgradeTokenizer(max_vocab_size=40000)\n",
    "tokenizer2.build_vocab(corpus)\n",
    "\n",
    "# Tokenize a sentence\n",
    "tokens = tokenizer2.tokenize(\"this is a test sentence for the tokenizer.\")\n",
    "print(tokens)\n",
    "\n",
    "# Convert tokens back to string\n",
    "sentence = tokenizer2.convert_tokens_to_string(tokens)\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = '/home/luqiao/project/data/vocab40000-p.json'\n",
    "with open(vocab_file, 'w') as json_file:\n",
    "    json.dump(tokenizer2.vocab, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer (VOCAB 60000 with punctuations and case sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpgradeTokenizer2:\n",
    "    def __init__(self, max_vocab_size, punctuations=['.', ',', '!', '?', ':', ';', '-', '(', ')']):\n",
    "        self.vocab = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}\n",
    "        self.mask_token = '[MASK]'\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.punctuations = punctuations\n",
    "\n",
    "    def custom_tokenize(self, text):\n",
    "        # Generate a regex pattern that excludes specified punctuations\n",
    "        excluded_punctuations = ''.join(re.escape(p) for p in self.punctuations)\n",
    "        pattern = r\"\\b\\w+'?\\w*|[^\\w\\s\" + excluded_punctuations + \"]\"\n",
    "\n",
    "        tokens = re.findall(pattern, text.lower())\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        word_counts = Counter(word for sentence in corpus for word in self.custom_tokenize(sentence))\n",
    "        for word, _ in word_counts.most_common(self.max_vocab_size - len(self.vocab)):\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return [self.vocab.get(word, self.vocab['[UNK]']) for word in self.custom_tokenize(text)]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        words = [list(self.vocab.keys())[list(self.vocab.values()).index(token)] for token in tokens]\n",
    "        sentence = ''\n",
    "        for word in words:\n",
    "            if word in self.punctuations:\n",
    "                sentence += word\n",
    "            else:\n",
    "                if sentence and not sentence.endswith(' '):\n",
    "                    sentence += ' '\n",
    "                sentence += word\n",
    "        return sentence\n",
    "\n",
    "# Initialize your tokenizer\n",
    "tokenizer = UpgradeTokenizer2(max_vocab_size=60000)  # Adjust max_vocab_size as needed\n",
    "\n",
    "def read_corpus(folder_path):\n",
    "    corpus = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):  # Ensure it's a text file\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                corpus.append(file.read())\n",
    "    return corpus\n",
    "\n",
    "# Specify the folder path containing your text files\n",
    "folder_path = '/root/autodl-tmp/train_data'\n",
    "\n",
    "# Read the corpus from text files\n",
    "corpus = read_corpus(folder_path)\n",
    "\n",
    "# Build vocabulary using the corpus\n",
    "tokenizer.build_vocab(corpus)\n",
    "\n",
    "# Tokenize a sentence\n",
    "tokens = tokenizer.tokenize(\"this is a test sentence for the tokenizer. it's waht you're! what?\")\n",
    "print(tokens)\n",
    "\n",
    "# Convert tokens back to string\n",
    "sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(text):\n",
    "    punctuations=['.', ',', '!', '?', ':', ';', '-', '(', ')']\n",
    "    # text = text.lower()  # Convert text to lower case\n",
    "    # Escape punctuations for regular expression\n",
    "    escaped_punctuations = [re.escape(p) for p in punctuations]\n",
    "    # Pattern for words or punctuation\n",
    "    pattern = r'\\w+|' + '|'.join(escaped_punctuations)\n",
    "    \n",
    "    tokens = re.findall(pattern, text)\n",
    "    return tokens\n",
    "\n",
    "def convert_tokens_to_string(tokens):\n",
    "    punctuations=['.', ',', '!', '?', ':', ';', '-', '(', ')']\n",
    "    words = [list(tokenizer2.vocab.keys())[list(tokenizer2.vocab.values()).index(token)] for token in tokens]\n",
    "    sentence = ''\n",
    "    for word in words:\n",
    "        if word in punctuations:\n",
    "            sentence += word  # Add punctuation without space\n",
    "        else:\n",
    "            if sentence and not sentence.endswith(' '):\n",
    "                sentence += ' '  # Add space before word if it's not the start of the sentence\n",
    "            sentence += word\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = '/root/project/vocab60000-latest.json'\n",
    "with open(vocab_file, 'w') as json_file:\n",
    "    json.dump(tokenizer.vocab, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = custom_tokenize(\"this is a test sentence for the tokenizer.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tokens_to_string([35, 15, 11, 699, 2723, 17, 7, 1, 5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
